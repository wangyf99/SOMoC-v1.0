{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf05108-9c9b-4ba7-99fe-cc7daea3b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: LIDeB UNLP\n",
    "\"\"\"\n",
    "# iRaPCA Clustering is a clustering strategy based on an iterative combination \n",
    "# of the random subspace approach (feature bagging), dimensionality reduction \n",
    "# through Principal Component Analysis (PCA) and the k-means algorithm.\n",
    "\n",
    "##################################### Import packages ####################################\n",
    "###########################################################################################\n",
    "# The following packages are required: SKlearn, RDKit, Molvs, Mordred, validclust and Plotly.\n",
    "# Please, meake sure you have them installed before running the program.\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import os\n",
    "from statistics import mean, stdev\n",
    "from multiprocessing import freeze_support\n",
    "from mordred import Calculator, descriptors\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, pairwise_distances\n",
    "from validclust import dunn\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as plt1\n",
    "import random\n",
    "from datetime import date\n",
    "from molvs import Standardizer\n",
    "import warnings\n",
    "\n",
    "###################################### CONFIGURATION ######################################\n",
    "###########################################################################################\n",
    "\n",
    "# Folder with files\n",
    "# directory = str(Path(r\"C:\\Users\\adm\\Desktop\\Pruebas\\Clustering\\dataset_focal_adhesion\"))\n",
    "\n",
    "# Input file could be a .CSV file with one molecule per line in SMILES format.\n",
    "# or a .TXT file with molecular descriptors. Your file should have a column called 'NAME'\n",
    "input_file = \"focal_adhesion.csv\"\n",
    "\n",
    "# If the molecular descriptors were previously calculated \n",
    "available_molecular_descriptors = False\n",
    "\n",
    "# If the SMILES should be standardized or not\n",
    "smiles_standardization = True      \n",
    "# Ignore error in SMILES\n",
    "ignore_error = True\n",
    "\n",
    "# Threshold variance\n",
    "threshold_variance = 0.05\n",
    "\n",
    "# If you want a random seed\n",
    "random_subspace_seed= False\n",
    "\n",
    "# Nº of subsets and Nº descriptors per subset\n",
    "num_subsets = 100 \n",
    "num_descriptors = 200 \n",
    "\n",
    "# Correlation coefficient and Threshold correlation\n",
    "coef_correlacion = \"pearson\"  # it can be \"pearson\", \"kendall\" y \"spearman\"\n",
    "threshold_correlation = 0.4          \n",
    "\n",
    "# Min and Max Nº of descriptors for subset\n",
    "min_desc_subset = 4\n",
    "max_desc_subset = 25\n",
    "\n",
    "# Min and Max Nº of clusters by round\n",
    "min_n_clusters = 2 \n",
    "max_n_clusters = 25 \n",
    "range_n_clusters = list(range(2,25,1)) # range(start, stop, step)\n",
    "\n",
    "# Max relation \"cluster/total\" and Max Nº of rounds\n",
    "max_ratio_cluster_total = 0.30\n",
    "max_round = 5\n",
    "\n",
    "# Nº of Principal Component Analysis \n",
    "num_pca = 2\n",
    "\n",
    "# Plots \n",
    "plot_silhouette = True\n",
    "plot_scatter = True\n",
    "plot_sunburnt = True\n",
    "plot_bar = True\n",
    "\n",
    "# plot configuration\n",
    "plot_format = \"svg\"     # it can be png, svg, jpeg, webp\n",
    "plot_height = 800       # plot height in pixeles\n",
    "plot_width = 800        # plot width in pixeles\n",
    "plot_scale = 2          # Multiply title/legend/axis/canvas sizes by this factor\n",
    "\n",
    "config = {'toImageButtonOptions': {'format': plot_format, \n",
    "    'height': plot_height,'width': plot_width,'scale': plot_scale}}\n",
    "    \n",
    "#%%\n",
    "\n",
    "#################################### Helper functions #####################################\n",
    "###########################################################################################\n",
    "\n",
    "def Make_dir(dirName: str):\n",
    "    \"\"\"Create a directory and not fail if it already exist\"\"\"\n",
    "    try:\n",
    "        os.makedirs(directory + \"\\\\\" + dirName)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "def Get_name(archive):\n",
    "    \"\"\"Strip path and extension to return the name of a file\"\"\"\n",
    "    return os.path.basename(archive).split('.')[0]\n",
    "\n",
    "#%%\n",
    "\n",
    "# Reading descriptor or SMILES files\n",
    "\n",
    "def Get_input_data(directory, input_file, available_molecular_descriptors):\n",
    "    \"\"\"Get data from user input or use test dataset\"\"\"\n",
    "    \n",
    "    if available_molecular_descriptors:\n",
    "        # molecular descriptors in a TXT file. Your file should have a column called 'NAME'\n",
    "        name = Get_name(directory + \"\\\\\" +  input_file)\n",
    "        uploaded_file_1 = pd.read_csv(directory + \"\\\\\" + input_file, sep='\\t', delimiter=None, header='infer', names=None)\n",
    "    else:  \n",
    "        name = Get_name(directory + \"\\\\\" +  input_file)\n",
    "        uploaded_file_1 = pd.read_csv(directory + \"\\\\\" +  input_file, delimiter=',', header = None)\n",
    "        if \"SMILES\" in uploaded_file_1.iloc[0].values:\n",
    "            new_header = uploaded_file_1.iloc[0]\n",
    "            uploaded_file_1 = uploaded_file_1[1:]\n",
    "            uploaded_file_1.columns = new_header\n",
    "        else:\n",
    "            uploaded_file_1.rename(columns = {0: 'SMILES'}, inplace = True)\n",
    "        print('-'*50)\n",
    "        print(\"Number of Molecules: \" + str(uploaded_file_1.shape[0]))\n",
    "        \n",
    "    return uploaded_file_1, name\n",
    "\n",
    "#%%\n",
    "# Standardization of molecules\n",
    "\n",
    "def Standardize_molecules(uploaded_file_1, rows_to_retain,smiles_standardization, ignore_error):\n",
    "    \"\"\"Standardize molecules using the MolVS package https://molvs.readthedocs.io/en/latest/\"\"\"\n",
    "    print('='*50)\n",
    "    print(\"**Standarize molecules**\\n\")\n",
    "    if smiles_standardization == True:\n",
    "        print('By default iRaPCA will standardize molecules before descriptor calculation. However, you can disable standardization by setting smiles_standardization=False.\\n')\n",
    "    else:\n",
    "        print('Standardization of molecules was skipped.\\n')\n",
    "    list_of_smiles = uploaded_file_1['SMILES']\n",
    "    time_start = time.time()\n",
    "    s = Standardizer()\n",
    "    standard_mol = []\n",
    "    problematic_smiles = []\n",
    "    \n",
    "    for i,molecule in enumerate(list_of_smiles, start = 1):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(molecule)\n",
    "            if smiles_standardization == True:\n",
    "                estandarizada = s.super_parent(mol) \n",
    "                standard_mol.append(estandarizada)\n",
    "                rows_to_retain.append(i -1)\n",
    "            else:\n",
    "                standard_mol.append(mol)\n",
    "        except:\n",
    "            problematic_smiles.append(i)\n",
    "\n",
    "    if ignore_error == False and len(problematic_smiles) > 0:\n",
    "        print(\"Oh no! There is a problem with descriptor calculation of some SMILES.\")\n",
    "        print(f\"Please check your SMILES number: {str(problematic_smiles)}\")\n",
    "        sys.exit()\n",
    "    else:\n",
    "        if len(problematic_smiles) > 0:\n",
    "            print(f\"Lines {str(problematic_smiles)} have problematic (or empty) SMILES. We have omitted them.\")\n",
    "\n",
    "    print(f\"{len(standard_mol)} molecules processed\")\n",
    "    print(f'Standardization took {round(time.time()-time_start)} seconds')\n",
    "    return standard_mol, rows_to_retain\n",
    "\n",
    "#%% \n",
    "\n",
    "### Reading/calculating molecular descriptors ###\n",
    "\n",
    "def clean_descriptors(uploaded_file_1):\n",
    "    \"\"\"Clean the descriptor file \"\"\"\n",
    "    print('='*50)\n",
    "    print(\"**Uploading molecular descriptor**\\n\") \n",
    "    descriptores = uploaded_file_1\n",
    "    molecules_names = descriptores['NAME'].tolist()\n",
    "    descriptores.drop(['NAME'], axis=1,inplace=True)\n",
    "    lista_nombres = []\n",
    "    for i,name in enumerate(molecules_names):\n",
    "        nombre = f'Molecule_{i+1}'\n",
    "        lista_nombres.append(nombre)\n",
    "    descriptores['NAME'] = lista_nombres\n",
    "    descriptores.set_index(\"NAME\",inplace=True)\n",
    "    descriptores = descriptores.reindex(sorted(descriptores.columns), axis=1)\n",
    "    descriptores.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    descriptores = descriptores.apply(pd.to_numeric, errors = 'coerce')\n",
    "    descriptores = descriptores.dropna(axis=0,how=\"all\")\n",
    "    descriptores = descriptores.dropna(axis=1)\n",
    "       \n",
    "    return descriptores\n",
    "\n",
    "def calculate_descriptors(standard_mol, rows_to_retain):\n",
    "    \"\"\"Calculate the 1613 conformational-independent molecular descriptors \n",
    "    by Mordred package https://github.com/mordred-descriptor/mordred\"\"\"\n",
    "    \n",
    "    print('='*50)\n",
    "    print(\"**Calculating Molecular descriptors**\\n\")\n",
    "\n",
    "    data1x = pd.DataFrame()\n",
    "    calc = Calculator(descriptors, ignore_3D=True) \n",
    "    time_start = time.time()\n",
    "    for i,mol in enumerate(standard_mol):\n",
    "        if __name__ == \"__main__\":\n",
    "                if mol != None:\n",
    "                    try:\n",
    "                        freeze_support()\n",
    "                        descriptor1 = calc(mol)\n",
    "                        resu = descriptor1.asdict()\n",
    "                        solo_nombre = {'NAME' : f'SMILES_{i+1}'}\n",
    "                        solo_nombre.update(resu)\n",
    "\n",
    "                        solo_nombre = pd.DataFrame.from_dict(data=solo_nombre,orient=\"index\")\n",
    "                        data1x = pd.concat([data1x, solo_nombre],axis=1, ignore_index=True)\n",
    "                        # print(\"Molecule  \" + str(i+1) +\"/\" + str(len(standard_mol)))   \n",
    "                        if smiles_standardization == False:\n",
    "                            rows_to_retain.append(i)\n",
    "                    except:\n",
    "                        print(\"Oh no! There is a problem with descriptor calculation of some SMILES.\")\n",
    "                        print(\"Please check your SMILES number: \" + str(i+1))\n",
    "                        sys.exit()\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    previuos_data = uploaded_file_1.iloc[rows_to_retain]\n",
    "\n",
    "    data1x = data1x.T\n",
    "    descriptores = data1x.set_index('NAME',inplace=False).copy()\n",
    "    descriptores = descriptores.reindex(sorted(descriptores.columns), axis=1)   \n",
    "    descriptores.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    descriptores = descriptores.apply(pd.to_numeric, errors = 'coerce') \n",
    "    descriptores = descriptores.dropna(axis=0,how=\"all\")\n",
    "    descriptores = descriptores.dropna(axis=1)\n",
    "    print(f'Descriptor calculation took {round(time.time()-time_start)} seconds')\n",
    "    return descriptores, previuos_data \n",
    "        \n",
    "\n",
    "#%%\n",
    "\n",
    "### Removing low variance descriptors ###\n",
    "\n",
    "def descriptores_baja_variancia(descriptores, vuelta, threshold_variance: float):\n",
    "    selector = VarianceThreshold(threshold_variance)       \n",
    "    selector.fit(descriptores)                              \n",
    "    descriptores_ok = descriptores[descriptores.columns[selector.get_support(indices=True)]]\n",
    "    if vuelta == 1:\n",
    "        print(f'{str(descriptores_ok.shape[1])} descriptors have passed the variance threshold')\n",
    "        print(\"=\"*50)\n",
    "        print(\"**Clustering**\\n\")\n",
    "          \n",
    "    return descriptores_ok\n",
    "\n",
    "#%%\n",
    "\n",
    "### Subsetting ###\n",
    "\n",
    "def generar_subset(descriptores_ok, num_subsets: int, coef_correlacion: str, threshold_correlation: float, vuelta):\n",
    "    subsets_ok=[]\n",
    "    i=0\n",
    "    while (i < num_subsets): \n",
    "        if random_subspace_seed == True:\n",
    "            subset= descriptores_ok.sample(num_descriptors,axis=1)\n",
    "        else:\n",
    "            subset= descriptores_ok.sample(num_descriptors,axis=1,random_state=i)  \n",
    "        corr_matrix = subset.corr(coef_correlacion).abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > threshold_correlation)]\n",
    "        curado=subset.drop(subset[to_drop], axis=1)\n",
    "        total_molec_subset = curado.shape[0]\n",
    "        i = i+1\n",
    "        subsets_ok.append(curado)\n",
    "  \n",
    "    return subsets_ok, total_molec_subset\n",
    "\n",
    "#%%\n",
    "\n",
    "### Normalization ###\n",
    "\n",
    "def normalizar_descriptores(subset):\n",
    "    descriptores_sin_normalizar = pd.DataFrame(subset)\n",
    "    scaler = MinMaxScaler()\n",
    "    descriptores_normalizados = pd.DataFrame(scaler.fit_transform(descriptores_sin_normalizar)) \n",
    "    return descriptores_normalizados\n",
    "\n",
    "#%%\n",
    "### Clustering ###\n",
    "\n",
    "def PCA_clustering(descriptores_normalizados, range_n_clusters, num_pca: float, siluetas):\n",
    "\n",
    "    sil_coef_grafica = []\n",
    "    for n_clusters in range_n_clusters:\n",
    "        pca = PCA(n_components = num_pca)\n",
    "        pcas = pd.DataFrame(pca.fit_transform(descriptores_normalizados))\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(pcas)\n",
    "        silhouette_avg = silhouette_score(pcas, cluster_labels)\n",
    "        sil_coef_grafica.append(silhouette_avg)\n",
    "\n",
    "    siluetas.append(sil_coef_grafica)\n",
    "\n",
    "    return siluetas\n",
    "\n",
    "def clustering(subsets_ok, min_desc_subset: int, max_desc_subset: int, range_n_clusters, num_pca: int):\n",
    "    siluetas = []\n",
    "    subsets_seleccionados = []\n",
    "    for i, subset in enumerate(subsets_ok):\n",
    "        \n",
    "        if min_desc_subset < len(subset.columns) < max_desc_subset:\n",
    "            descriptores_normalizados = normalizar_descriptores(subset)\n",
    "            if max_n_clusters > len(descriptores_normalizados.index):\n",
    "                \n",
    "                range_n_clusters = list(range(min_n_clusters,len(descriptores_normalizados.index),1))\n",
    "            siluetas = PCA_clustering(descriptores_normalizados, range_n_clusters, num_pca, siluetas)\n",
    "            subsets_seleccionados.append(i)\n",
    "    \n",
    "    tabla_final = pd.DataFrame(siluetas).T\n",
    "    tabla_final.columns = subsets_seleccionados\n",
    "    tabla_final.index = range_n_clusters\n",
    "    return tabla_final, subsets_seleccionados\n",
    "\n",
    "#%%\n",
    "\n",
    "### Plot Silhouette coefficient vs K for each subset ###\n",
    "\n",
    "def grafica_silhouette(subsets_seleccionados,tabla_final,num_pca: int, range_n_clusters, vuelta, threshold_correlation: float):\n",
    "    \n",
    "    if plot_silhouette:\n",
    "        fig = go.Figure()\n",
    "        for num in subsets_seleccionados:\n",
    "            fig.add_trace(go.Scatter(x=range_n_clusters, y=tabla_final[num], \n",
    "                            mode='lines+markers', name= f'Subset {num}', \n",
    "                            hovertemplate = \"Subset = %s<br>Clusters = %%{x}<br>Silhouette = %%{y} <extra></extra>\" % num))\n",
    "        \n",
    "        fig.update_layout(title = 'Number of clusters for K-means vs Silhouette coefficient',\n",
    "                          plot_bgcolor = 'rgb(256,256,256)',\n",
    "                          title_font = dict(size=25, family='Calibri', color='black'),\n",
    "                          legend_title_text = \"Subsets\", \n",
    "                          legend_title_font = dict(size=18, family='Calibri', color='black'),\n",
    "                          legend_font = dict(size=15, family='Calibri', color='black'))\n",
    "        fig.update_xaxes(title_text='K (number of clusters)', range = [1.5, 20.5],\n",
    "                         showline=True, linecolor='black', gridcolor='lightgrey', zerolinecolor = 'lightgrey',\n",
    "                         tickfont=dict(family='Arial', size=16, color='black'),\n",
    "                         title_font = dict(size=20, family='Calibri', color='black'))\n",
    "        fig.update_yaxes(title_text='SIL coefficient',\n",
    "                         showline=True, linecolor='black', gridcolor='lightgrey', zerolinecolor = 'lightgrey',\n",
    "                         tickfont=dict(family='Arial', size=16, color='black'),\n",
    "                         title_font = dict(size=20, family='Calibri', color='black'))\n",
    "\n",
    "        plotly.offline.plot(fig, filename= f'{directory}\\\\results_iRaPCA_{name}\\\\SIL_round_{vuelta}_PCAs_{num_pca}_coef_{threshold_correlation}_{name}.html', config=config)\n",
    "    return\n",
    "#%%\n",
    "\n",
    "### Clusters adittional information ###\n",
    "\n",
    "def moleculas_en_cluster_PCA_clustering(subset_seleccionado, num_pca: int, cluster_mejor: int, subset_mejor: int, clusters_padre, vuelta, descriptores):\n",
    "    \n",
    "    subset_seleccionado_normalizado = normalizar_descriptores(subset_seleccionado)\n",
    "    pca = PCA(n_components = num_pca)\n",
    "    pcas = pd.DataFrame(pca.fit_transform(subset_seleccionado_normalizado))\n",
    "    pcas = pcas.set_index(subset_seleccionado.index)\n",
    "    for j,i in enumerate(range(num_pca),start=1):\n",
    "        pcas.rename(columns = {i: \"PCA_\" + str(j)}, inplace=True)\n",
    "    kmeans_new = KMeans(n_clusters=cluster_mejor, random_state=10).fit(pcas)\n",
    "    df_molecula_cluster_actual = pd.DataFrame(kmeans_new.fit_predict(pcas))\n",
    "    df_molecula_cluster_actual.rename(columns={0: 'CLUSTER'},inplace = True)\n",
    "    df_molecula_cluster_actual['CLUSTER'] = df_molecula_cluster_actual['CLUSTER'] + 1    \n",
    "    df_molecula_cluster_actual.index = subset_seleccionado.index.tolist()\n",
    "    \n",
    "    # ordeno los cluster por tamaño para que el mas grande sea el 1\n",
    "    cluster_ordenados = []\n",
    "    df_contado = pd.DataFrame(df_molecula_cluster_actual['CLUSTER'].value_counts())\n",
    "    df_contado['cluster_nuevo'] = list(range(1, len(df_contado)+1))\n",
    "    for j in range(len(df_molecula_cluster_actual)):\n",
    "        for i in range(1, len(df_contado)+1):\n",
    "            if df_molecula_cluster_actual['CLUSTER'][j] == i:\n",
    "                cluster_ordenados.append(df_contado['cluster_nuevo'][i])\n",
    "    df_molecula_cluster_actual['CLUSTER'] = cluster_ordenados    \n",
    "    \n",
    "    if vuelta == 1:\n",
    "        df_cluster_padre = pd.DataFrame(pd.Series([cluster_actual for cluster_actual in df_molecula_cluster_actual['CLUSTER']]))\n",
    "    else:\n",
    "        lista_nombre_cluster_padre = [[str(clusters_padre), str(cluster_actual)] for cluster_actual in df_molecula_cluster_actual['CLUSTER']]\n",
    "        df_cluster_padre = pd.DataFrame(pd.Series(['.'.join(nombre_cluster_padre) for nombre_cluster_padre in lista_nombre_cluster_padre]))\n",
    "    df_cluster_padre.rename(columns={0: 'Cluster, padre'},inplace = True)\n",
    "    df_cluster_padre.index = df_molecula_cluster_actual.index.values\n",
    "\n",
    "    df_cluster_con_cluster_padre = pd.merge(df_molecula_cluster_actual, df_cluster_padre, left_index = True, right_index= True)\n",
    "    df_subset_PCA = pd.merge(subset_seleccionado, pcas, left_index = True, right_index= True)\n",
    "    moleculas_cluster = pd.merge(df_subset_PCA, df_cluster_con_cluster_padre, left_index = True, right_index= True)\n",
    "\n",
    "    final_conteo = pd.DataFrame(moleculas_cluster['Cluster, padre'].value_counts())\n",
    "    final_conteo.rename(columns = {'Cluster, padre':'Molecules'}, inplace = True)\n",
    "    final_conteo.index.names = ['Cluster']\n",
    "    final_conteo['Relacion'] = final_conteo['Molecules']/descriptores.shape[0]\n",
    "    return pcas, moleculas_cluster, final_conteo\n",
    "\n",
    "#%%\n",
    "\n",
    "### Scatter plot with PCAs for each selected subset and K ###\n",
    "\n",
    "def grafica_scatter(moleculas_cluster,subset_mejor,cluster_mejor, vuelta):\n",
    "    if plot_scatter:\n",
    "        tabla_final_moleculas = moleculas_cluster.copy()\n",
    "        tabla_final_moleculas.rename(columns = {'PCA_1': 'PC_1', 'PCA_2': 'PC_2', 'Cluster, padre': 'Cluster'}, inplace = True)\n",
    "        tabla_final_moleculas['Cluster'] = tabla_final_moleculas['Cluster'].astype(str)\n",
    "        \n",
    "        fig2 = plt1.scatter(tabla_final_moleculas, x = 'PC_1', y = 'PC_2', color = 'Cluster',\n",
    "                           hover_name = tabla_final_moleculas.index, \n",
    "                           title = f'Scatter Plot of PC 1 vs PC 2 for subset {subset_mejor} and K {cluster_mejor}')\n",
    "        fig2.update_layout(legend_title=\"Cluster\", plot_bgcolor = 'rgb(256,256,256)',\n",
    "                           title_font = dict(size=25, family='Calibri', color='black'),\n",
    "                           legend_title_font = dict(size=18, family='Calibri', color='black'),\n",
    "                           legend_font = dict(size=15, family='Calibri', color='black'))\n",
    "        fig2.update_traces(marker=dict(size=15, line=dict(width=1)))\n",
    "        fig2.update_xaxes(title_text=\"PC 1\", showline=True, linecolor='black', \n",
    "                          gridcolor='lightgrey', zerolinecolor = 'lightgrey',\n",
    "                          tickfont=dict(family='Arial', size=16, color='black'),\n",
    "                          title_font = dict(size=20, family='Calibri', color='black'))\n",
    "        fig2.update_yaxes(title_text=\"PC 2\", showline=True, linecolor='black', \n",
    "                          gridcolor='lightgrey', zerolinecolor = 'lightgrey',\n",
    "                          tickfont=dict(family='Arial', size=16, color='black'),\n",
    "                          title_font = dict(size=20, family='Calibri', color='black'))     \n",
    "        plotly.offline.plot(fig2, filename= f'{directory}\\\\results_iRaPCA_{name}\\\\Scatterplot_PCs_round_{vuelta}_cluster_{cluster_mejor}_subset_{subset_mejor}_{name}.html', config=config)\n",
    "    return\n",
    "      \n",
    "#%%\n",
    "\n",
    "### Random cluster evaluations ###\n",
    "\n",
    "def cluster_random(pcas, molec_name,cluster_mejor):\n",
    "    compilado_silhoutte = []\n",
    "    compilado_db = []\n",
    "    compilado_ch = []\n",
    "    compilado_dunn = []\n",
    "    \n",
    "    for i in range(500):\n",
    "        random.seed(a=i, version=2)\n",
    "        random_clusters = []\n",
    "        for x in molec_name:\n",
    "            random_clusters.append(random.randint(0,cluster_mejor-1))\n",
    "        silhouette_random = silhouette_score(pcas, np.ravel(random_clusters))\n",
    "        compilado_silhoutte.append(silhouette_random)\n",
    "        db_random = davies_bouldin_score(pcas, np.ravel(random_clusters))\n",
    "        compilado_db.append(db_random)\n",
    "        ch_random = calinski_harabasz_score(pcas, np.ravel(random_clusters))\n",
    "        compilado_ch.append(ch_random)\n",
    "        dist_dunn = pairwise_distances(pcas)\n",
    "        dunn_randome = dunn(dist_dunn, np.ravel(random_clusters))\n",
    "        compilado_dunn.append(dunn_randome)\n",
    "\n",
    "    sil_random = round(mean(compilado_silhoutte),4)\n",
    "    sil_random_st = str(round(stdev(compilado_silhoutte),4))\n",
    "    db_random = round(mean(compilado_db),4)\n",
    "    db_random_st = str(round(stdev(compilado_db),4))\n",
    "    ch_random = round(mean(compilado_ch),4)\n",
    "    ch_random_st = str(round(stdev(compilado_ch),4))\n",
    "    dunn_random = round(mean(compilado_dunn),4)\n",
    "    dunn_random_st = str(round(stdev(compilado_dunn),4))\n",
    "\n",
    "    return sil_random, sil_random_st, db_random, db_random_st, ch_random, ch_random_st, dunn_random, dunn_random_st\n",
    "\n",
    "\n",
    "### Clustering performance determination ###\n",
    "\n",
    "def coeficientes_clustering(pcas, df_molecula_cluster_actual, cluster_mejor, molec_name,vuelta):\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "\n",
    "    sil_random, sil_random_st, db_random, db_random_st, ch_random, ch_random_st, dunn_random, dunn_random_st = cluster_random(pcas, molec_name,cluster_mejor)\n",
    "    silhouette_avg = round(silhouette_score(pcas, np.ravel(df_molecula_cluster_actual)),4)\n",
    "    gmm = GaussianMixture(n_components=cluster_mejor, init_params='kmeans')\n",
    "    gmm.fit(pcas)\n",
    "    db_score = round(davies_bouldin_score(pcas, np.ravel(df_molecula_cluster_actual)),4)\n",
    "    ch_score = round(calinski_harabasz_score(pcas, np.ravel(df_molecula_cluster_actual)),4)\n",
    "    dist_dunn = pairwise_distances(pcas)\n",
    "    dunn_score = round(dunn(dist_dunn, np.ravel(df_molecula_cluster_actual)),4)\n",
    "    if vuelta == 1:\n",
    "       print(f'\\nThe Silhouette score is: {silhouette_avg}')\n",
    "       print(f'The Silhouette Score for random cluster is: {sil_random}')\n",
    "    validation_round = [vuelta,silhouette_avg, sil_random, sil_random_st, db_score, db_random, db_random_st,ch_score, ch_random, ch_random_st,dunn_score, dunn_random, dunn_random_st]\n",
    "\n",
    "    return validation_round\n",
    "\n",
    "#%%\n",
    "\n",
    "### Indexes ###\n",
    "\n",
    "def getIndexes(df, value):\n",
    "    ''' Get index positions of value in dataframe as a tuple\n",
    "    first the subset,then the cluster '''\n",
    "\n",
    "    result = df.isin([value])\n",
    "    seriesObj = result.any()\n",
    "    columnNames = list(seriesObj[seriesObj == True].index)\n",
    "    for col in columnNames:\n",
    "        rows = list(result[col][result[col] == True].index)\n",
    "        for row in rows:\n",
    "            posicion = (row, col)\n",
    "    return posicion\n",
    "\n",
    "#%%\n",
    "\n",
    "### Hierarchical Clustering ###\n",
    "\n",
    "def clusters_con_mayor_porcentaje(lista_final_conteo, max_ratio_cluster_total):\n",
    "    lista_cluster_para_seguir = []\n",
    "    lista_cluster_padres = []\n",
    "    for final_conteo_ in lista_final_conteo:\n",
    "        clusters_para_seguir = []\n",
    "        for index, row in final_conteo_.iterrows():\n",
    "            if row['Relacion'] > max_ratio_cluster_total:\n",
    "                clusters_para_seguir.append(index)\n",
    "                lista_cluster_padres.append(index)\n",
    "        lista_cluster_para_seguir.append(clusters_para_seguir)\n",
    "    return lista_cluster_para_seguir, lista_cluster_padres\n",
    "\n",
    "def asignar_moleculas_para_RDCPCA(lista_cluster_para_seguir, lista_cluster_moleculas, moleculas_compiladas, vuelta):\n",
    "    lista_nuevas_moleculas = []\n",
    "    for p, cluster_para_seguir_ in enumerate(lista_cluster_para_seguir):\n",
    "        if cluster_para_seguir_ is not None:\n",
    "            for cluster_ in cluster_para_seguir_:\n",
    "                nuevas_moleculas = []\n",
    "                for index, row in lista_cluster_moleculas[p].iterrows():\n",
    "                    if row['Cluster, padre'] == cluster_:\n",
    "                        nuevas_moleculas.append(index)\n",
    "                        if vuelta == max_round:\n",
    "                            moleculas_compiladas[index] = row['Cluster, padre']\n",
    "                lista_nuevas_moleculas.append(nuevas_moleculas)\n",
    "\n",
    "    for cluster_moleculas_ in lista_cluster_moleculas:\n",
    "        for index, row in cluster_moleculas_.iterrows():\n",
    "            agregar_o_no = any([index in nuevas_moleculas_ for nuevas_moleculas_ in lista_nuevas_moleculas])\n",
    "            if agregar_o_no == False:\n",
    "                moleculas_compiladas[index] = row['Cluster, padre']\n",
    "    return lista_nuevas_moleculas, moleculas_compiladas\n",
    "\n",
    "#%%\n",
    "\n",
    "### Sunburn plot of all the molecules ###\n",
    "\n",
    "def sunburn_plot(sunburnt):\n",
    "    if plot_sunburnt: \n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "        sunburnt.insert(loc = 0, column = 'All', value = 'All')\n",
    "        sunburnt = sunburnt.fillna(' ')\n",
    "        sunburnt['Molecules'] = 1\n",
    "        \n",
    "        fig3 = plt1.sunburst(sunburnt, path = sunburnt.iloc[:,0:-1], values = 'Molecules')\n",
    "        fig3.update_layout(title = \"Sunburst Plot\", title_x=0.5,\n",
    "                           title_font = dict(size=25, family='Calibri', color='black'))\n",
    "        fig3.update_layout(margin = dict(t=60,r=20,b=20,l=20), autosize = True)\n",
    "        \n",
    "        plotly.offline.plot(fig3, filename= f'{directory}\\\\results_iRaPCA_{name}\\\\Sunburst_{name}.html', config=config)\n",
    "    return\n",
    "\n",
    "#%%\n",
    " \n",
    "### Bar plot of molecule distribution ###\n",
    "\n",
    "def bar_plot_counts(dataframe_final_1):\n",
    "    if plot_bar: \n",
    "        fig4 = plt1.bar(dataframe_final_1, x = dataframe_final_1.index.get_level_values(0), y = 'Molecules', \n",
    "                       color = dataframe_final_1.index.get_level_values(0))\n",
    "        \n",
    "        fig4.update_layout(legend_title=\"Cluster\", plot_bgcolor = 'rgb(256,256,256)',\n",
    "                           legend_title_font = dict(size=18, family='Calibri', color='black'),\n",
    "                           legend_font = dict(size=15, family='Calibri', color='black'))\n",
    "        fig4.update_xaxes(title_text='Cluster', showline=True, linecolor='black', \n",
    "                          gridcolor='lightgrey', zerolinecolor = 'lightgrey',\n",
    "                          tickfont=dict(family='Arial', size=16, color='black'),\n",
    "                          title_font = dict(size=20, family='Calibri', color='black'))\n",
    "        fig4.update_yaxes(title_text='Amount of molecules', showline=True, linecolor='black', \n",
    "                          gridcolor='lightgrey', zerolinecolor = 'lightgrey',\n",
    "                          tickfont=dict(family='Arial', size=16, color='black'),\n",
    "                          title_font = dict(size=20, family='Calibri', color='black'))\n",
    "        plotly.offline.plot(fig4, filename= f'{directory}\\\\results_iRaPCA_{name}\\\\Barplot_{name}.html', config=config)\n",
    "    return \n",
    "\n",
    "#%%\n",
    "### Settings file ###\n",
    "\n",
    "def setting_info(vuelta,dataframe_final_1, total_time):\n",
    "\n",
    "    today = date.today()\n",
    "    fecha = today.strftime(\"%d/%m/%Y\")\n",
    "    settings = []\n",
    "    settings.append([\"Date clustering was performed: \" , fecha])\n",
    "    settings.append([\"Seetings:\",\"\"])\n",
    "    settings.append([\"Threshold variance:\", str(threshold_variance)])\n",
    "    settings.append([\"Random seed:\", str(random_subspace_seed)])\n",
    "    settings.append([\"Number of subsets:\", str(num_subsets)])\n",
    "    settings.append([\"Number of descriptors by subset:\", str(num_descriptors)])\n",
    "    settings.append([\"Correlation coefficient:\", str(coef_correlacion)])\n",
    "    settings.append([\"Correlation threshold:\", str(threshold_correlation)])\n",
    "    settings.append([\"Min number of descriptors by subset:\", str(min_desc_subset)])\n",
    "    settings.append([\"Max number of descriptors by subset:\", str(max_desc_subset)])\n",
    "    settings.append([\"Min number of clusters by round:\", str(min_n_clusters)])\n",
    "    settings.append([\"Max number of clusters by round:\", str(max_n_clusters)])\n",
    "    settings.append([\"Max relation 'cluster/total':\", str(max_ratio_cluster_total)])\n",
    "    settings.append([\"Max number of rounds:\", str(max_round)])\n",
    "    settings.append([\"PCAs:\", str(num_pca)])\n",
    "    settings.append([\"\",\"\"])\n",
    "    settings.append([\"Results:\",\"\"])\n",
    "    settings.append([\"Total rounds :\", str(vuelta)])\n",
    "    settings.append([\"Total clusters :\", str(len(dataframe_final_1))])\n",
    "    settings.append([\"\",\"\"])\n",
    "    settings.append([\"Total running time : \", total_time])\n",
    "    settings.append([\"To cite the application, please reference: \",\"XXXXXXXXXXX\"])   \n",
    "    settings_df = pd.DataFrame(settings)\n",
    "    \n",
    "    settings_df.to_csv(f'{directory}\\\\results_iRaPCA_{name}\\\\Clustering_setting_{name}.csv',index=False,header=False)\n",
    "    return \n",
    "\n",
    "#%%\n",
    "\n",
    "####################################### iRaPCA main #######################################\n",
    "###########################################################################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    lista_nuevas_moleculas = [1]\n",
    "    vuelta = 1\n",
    "    moleculas_compiladas = {}\n",
    "    todos_silhouette = []\n",
    "    lista_cluster_padres = ['']\n",
    "    lista_cluster_moleculas = []\n",
    "    lista_descriptores = []\n",
    "    validation_all = []\n",
    "    rows_to_retain = []\n",
    "    uploaded_file_1, name = Get_input_data(directory, input_file, available_molecular_descriptors)\n",
    "    \n",
    "    Make_dir(f'results_iRaPCA_{name}') # Create output dir\n",
    "    \n",
    "    if available_molecular_descriptors:\n",
    "        descriptores = clean_descriptors(uploaded_file_1)\n",
    "    else:\n",
    "        standard_mol, rows_to_retain = Standardize_molecules(uploaded_file_1, rows_to_retain,smiles_standardization, ignore_error)\n",
    "        descriptores, previuos_data = calculate_descriptors(standard_mol, rows_to_retain)\n",
    "        \n",
    "    lista_descriptores.append(descriptores)\n",
    "    \n",
    "    while len(lista_nuevas_moleculas)>0 and vuelta <= max_round:\n",
    "    \n",
    "        lista_subsets_ok = []\n",
    "        lista_tablas_finales = []\n",
    "        lista_final_conteo = []\n",
    "        lista_subsets_seleccionados = []\n",
    "        lista_total_molec_subset =[]\n",
    "        sunburnt_nuevos = pd.Series(dtype = 'float64')\n",
    "    \n",
    "        for descriptores_ in lista_descriptores:\n",
    "            descriptores_ok = descriptores_baja_variancia(descriptores_, vuelta, threshold_variance)\n",
    "            subsets_ok, total_molec_subset = generar_subset(descriptores_ok, num_subsets, \n",
    "                                                            coef_correlacion, threshold_correlation,vuelta)\n",
    "            tabla_final, subsets_seleccionados = clustering(subsets_ok, min_desc_subset, \n",
    "                                                            max_desc_subset, range_n_clusters, num_pca)\n",
    "    \n",
    "            lista_subsets_ok.append(subsets_ok)\n",
    "            lista_total_molec_subset.append(total_molec_subset)\n",
    "            lista_tablas_finales.append(tabla_final)\n",
    "            lista_subsets_seleccionados.append(subsets_seleccionados)\n",
    "                \n",
    "        lista_cluster_moleculas = []\n",
    "        for j, tabla_final_ in enumerate(lista_tablas_finales):\n",
    "            try:\n",
    "                silhouette_max = tabla_final_.values.max()\n",
    "                cluster_mejor, subset_mejor = getIndexes(tabla_final_, silhouette_max)\n",
    "                subset_mejor_sil = lista_subsets_ok[j][subset_mejor]\n",
    "                pcas, cluster_moleculas, final_conteo = moleculas_en_cluster_PCA_clustering(subset_mejor_sil, \n",
    "                                                        num_pca, cluster_mejor, subset_mejor, lista_cluster_padres[j], vuelta, descriptores)\n",
    "                todos_silhouette.append(silhouette_max)\n",
    "            except ValueError:\n",
    "                if vuelta == 1:\n",
    "                    print(f'For the selected Threshold correlation filter ({threshold_correlation}) none of the subsets have between {min_desc_subset} and {max_desc_subset} descriptors in round {vuelta}')\n",
    "                    sys.exit()\n",
    "                else:\n",
    "                    for i, cluster_moleculas_ in enumerate(lista_cluster_moleculas):\n",
    "                        for index, row in cluster_moleculas_.iterrows():\n",
    "                            moleculas_compiladas[index] = row['Cluster, padre']\n",
    "                    print(f'For the selected Threshold correlation filter ({threshold_correlation}) none of the subsets have between {min_desc_subset} and {max_desc_subset} descriptors in round {vuelta}')\n",
    "                    sys.exit()\n",
    "            \n",
    "            print(f\"**Round: {vuelta}**\")\n",
    "            print(\"- Subsets with a number of descriptors between the limits: \" + str(len(lista_subsets_seleccionados[j])))\n",
    "            if vuelta != 1:\n",
    "                print(\"- The subset has: \" + str(lista_total_molec_subset[j]) + \" molecules\")\n",
    "            print(\"- The average number of descriptors by subset is: \" + str(round(mean([x.shape[1] for x in lista_subsets_ok[j]]),2)))\n",
    "            grafica_silhouette(lista_subsets_seleccionados[j],tabla_final_, num_pca, \n",
    "                               range_n_clusters,vuelta, threshold_correlation)\n",
    "            grafica_scatter(cluster_moleculas,subset_mejor,cluster_mejor, vuelta)\n",
    "            print(f'Maximum coefficient of silhouette was obtained in the subset {subset_mejor} with {cluster_mejor} clusters')\n",
    "    \n",
    "            if vuelta == 1:\n",
    "                sunburnt = pd.DataFrame(cluster_moleculas['Cluster, padre'])\n",
    "            else:\n",
    "                sunburnt_agregar = cluster_moleculas['Cluster, padre']\n",
    "                sunburnt_nuevos = pd.concat([sunburnt_nuevos, sunburnt_agregar], axis = 0)\n",
    "            validation_round = coeficientes_clustering(pcas, cluster_moleculas['CLUSTER'], \n",
    "                                                       cluster_mejor, cluster_moleculas.index,vuelta)\n",
    "            validation_all.append(validation_round)\n",
    "            lista_cluster_moleculas.append(cluster_moleculas)\n",
    "            lista_final_conteo.append(final_conteo)\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "        if vuelta != 1:\n",
    "            sunburnt_nuevos = sunburnt_nuevos.to_frame()\n",
    "            sunburnt_nuevos.rename(columns={0: f'Cluster, padre, V{vuelta}'},inplace = True)\n",
    "            sunburnt = pd.concat([sunburnt,sunburnt_nuevos], axis = 1)\n",
    "    \n",
    "        lista_cluster_para_seguir, lista_cluster_padres = clusters_con_mayor_porcentaje(lista_final_conteo, \n",
    "                                                                                        max_ratio_cluster_total)\n",
    "    \n",
    "        if len(lista_cluster_para_seguir) != 0:\n",
    "            lista_nuevas_moleculas, moleculas_compiladas = asignar_moleculas_para_RDCPCA(lista_cluster_para_seguir, \n",
    "                                                            lista_cluster_moleculas, moleculas_compiladas,vuelta)\n",
    "        else:\n",
    "            for i, cluster_moleculas_ in enumerate(lista_cluster_moleculas):\n",
    "                for index, row in cluster_moleculas_.iterrows():\n",
    "                    moleculas_compiladas[index] = row['Cluster, padre']\n",
    "            break\n",
    "            \n",
    "        lista_descriptores = []\n",
    "        for nuevas_moleculas_ in lista_nuevas_moleculas:\n",
    "            descriptores_nuevas_molec = []\n",
    "            for molec in nuevas_moleculas_:\n",
    "                row = descriptores.loc[molec]\n",
    "                descriptores_nuevas_molec.append(row)\n",
    "            descriptores_nuevas_molec = pd.DataFrame(descriptores_nuevas_molec)\n",
    "            lista_descriptores.append(descriptores_nuevas_molec)\n",
    "    \n",
    "        vuelta += 1\n",
    "        \n",
    "    dataframe_final = pd.DataFrame.from_dict(moleculas_compiladas, orient = 'index')\n",
    "    dataframe_final.rename(columns = {0: 'CLUSTER'}, inplace = True)\n",
    "    dataframe_final['key'] = dataframe_final.index\n",
    "    dataframe_final['key'] = dataframe_final['key'].str.split('_').str[1].astype(int)\n",
    "    dataframe_final = dataframe_final.sort_values('key', ascending=True).drop('key', axis=1)\n",
    "    \n",
    "    if available_molecular_descriptors:\n",
    "        dataframe_final.index.rename(\"NAME\", inplace = True)\n",
    "    else:\n",
    "        previuos_data.reset_index(drop = True, inplace = True)\n",
    "        dataframe_final.reset_index(drop = True, inplace = True)\n",
    "        dataframe_final = previuos_data.join(dataframe_final, how = 'right')    \n",
    "    \n",
    "    dataframe_final_1 = dataframe_final['CLUSTER'].value_counts().to_frame()\n",
    "    dataframe_final_1.rename(columns = {'CLUSTER': 'Molecules'}, inplace = True)\n",
    "     \n",
    "    validation_final = pd.DataFrame(validation_all)\n",
    "    validation_final.columns = [\"Round\",\"SIL score\", \"SIL random\", \"SD SIL random\", \n",
    "                                \"DB score\", \"DB random\", \"SD DB random\",\"CH score\", \"CH random\", \n",
    "                                \"SD CH random\", \"Dunn score\", \"Dunn random\", \"SD Dunn random\"]\n",
    "    \n",
    "    print('-'*50) \n",
    "    print('Clusterin has finished!')\n",
    "    if len(lista_nuevas_moleculas) == 0:\n",
    "        vuelta-=1\n",
    "        print(f'The {descriptores.shape[0]} molecules were distributed in {len(dataframe_final_1)} clusters \\n\\nThere is no more cluster with a relationship greater than selected value: {max_ratio_cluster_total}\\n')\n",
    "    else:\n",
    "        if vuelta == max_round+1:\n",
    "            vuelta-=1\n",
    "            print(f'The {descriptores.shape[0]} molecules were distributed in {len(dataframe_final_1)} clusters \\n\\nThe maximum number of rounds was reached {max_round}\\n')\n",
    "           \n",
    "    sunburn_plot(sunburnt)\n",
    "        \n",
    "    bar_plot_counts(dataframe_final_1)\n",
    "    \n",
    "    if available_molecular_descriptors:\n",
    "        dataframe_final.to_csv(f'{directory}\\\\results_iRaPCA_{name}\\\\Cluster_assignation_{name}.csv', index=True, header = True)\n",
    "    else:\n",
    "        dataframe_final.to_csv(f'{directory}\\\\results_iRaPCA_{name}\\\\Cluster_assignation_{name}.csv', index=False, header = True)\n",
    "    \n",
    "    dataframe_final_1.to_csv(f'{directory}\\\\results_iRaPCA_{name}\\\\Cluster_distributions_{name}.csv', index=True,header=True)\n",
    "    \n",
    "    validation_final.to_csv(f'{directory}\\\\results_iRaPCA_{name}\\\\Validation_indexes_{name}.csv',index=False,header=True)\n",
    "      \n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    total_time = \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "    print(total_time)\n",
    "    \n",
    "    setting_info(vuelta,dataframe_final_1, total_time)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
